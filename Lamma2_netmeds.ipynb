{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing required packages"
      ],
      "metadata": {
        "id": "ai0aU-nUvmJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8eL6h16AlZy"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q trl xformers wandb datasets einops gradio sentencepiece bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "LP-HTVFNv464"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os,torch, wandb, platform, gradio, warnings\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "xtRrIjqOGBJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check system specifications"
      ],
      "metadata": {
        "id": "xlrpCwXEv--O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_system_specs():\n",
        "    # Check if CUDA is available\n",
        "    is_cuda_available = torch.cuda.is_available()\n",
        "    print(\"CUDA Available:\", is_cuda_available)\n",
        "# Get the number of available CUDA devices\n",
        "    num_cuda_devices = torch.cuda.device_count()\n",
        "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
        "    if is_cuda_available:\n",
        "        for i in range(num_cuda_devices):\n",
        "            # Get CUDA device properties\n",
        "            device = torch.device('cuda', i)\n",
        "            print(f\"--- CUDA Device {i} ---\")\n",
        "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
        "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
        "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
        "    # Get CPU information\n",
        "    print(\"--- CPU Information ---\")\n",
        "    print(\"Processor:\", platform.processor())\n",
        "    print(\"System:\", platform.system(), platform.release())\n",
        "    print(\"Python Version:\", platform.python_version())\n",
        "print_system_specs()"
      ],
      "metadata": {
        "id": "b47t4zjGHR6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the model variable"
      ],
      "metadata": {
        "id": "ZEQ1rSBNwM80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre trained model\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = \"\"\n",
        "\n",
        "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
        "new_model = \"\""
      ],
      "metadata": {
        "id": "bjWX8dLwHofG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log into hugging face hub"
      ],
      "metadata": {
        "id": "2ya2stlVwTz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "wAfMtjrtIXf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZrTgZkjwadl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split=\"train[0:]\")\n",
        "dataset[\"About the Tablet\"][0]"
      ],
      "metadata": {
        "id": "Y7uV1ELVIa6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model and tokenizer"
      ],
      "metadata": {
        "id": "5U4Haidswo5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model(llama-2-7b-hf) and tokenizer\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token"
      ],
      "metadata": {
        "id": "-Dsq1StrIov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring"
      ],
      "metadata": {
        "id": "fjGx3qsdwtQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "Byb97LZOJLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"\")\n",
        "run = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")"
      ],
      "metadata": {
        "id": "G5zpp7bvooZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha= 8,\n",
        "    lora_dropout= 0.1,\n",
        "    r= 16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "oKFBezYgpH5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir= \"./results\",\n",
        "    num_train_epochs= 1,\n",
        "    per_device_train_batch_size= 8,\n",
        "    gradient_accumulation_steps= 2,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    save_steps= 1000,\n",
        "    logging_steps= 30,\n",
        "    learning_rate= 2e-4,\n",
        "    weight_decay= 0.001,\n",
        "    fp16= False,\n",
        "    bf16= False,\n",
        "    max_grad_norm= 0.3,\n",
        "    max_steps= -1,\n",
        "    warmup_ratio= 0.3,\n",
        "    group_by_length= True,\n",
        "    lr_scheduler_type= \"linear\",\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "O0fCVP1_qS7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"About the Tablet\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ],
      "metadata": {
        "id": "Xkx-tXDkqWqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yRqQywvAqaJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YMbBamC4qzq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream(user_prompt):\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
        "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
        "\n",
        "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "Ktx1J4NxrysC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream(\"User Qustion\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-F_VCNx6r_FP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}